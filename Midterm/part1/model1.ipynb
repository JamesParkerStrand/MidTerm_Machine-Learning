{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T05:55:36.964619Z",
     "start_time": "2026-02-12T05:55:19.356236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"dermatology.csv\", sep='\\t', encoding=\"utf-8-sig\", na_values=\"?\")\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "#only establishing variables\n",
    "Xfull = df.iloc[:,:-1].to_numpy()\n",
    "yDisease = df.iloc[:,-1].to_numpy()\n",
    "\n",
    "mean_value = np.nanmean(Xfull)\n",
    "\n",
    "# Replace NaNs with the mean\n",
    "Xfull[np.isnan(Xfull)] = mean_value\n",
    "\n",
    "print(\"Total NaNs in X_train:\", np.isnan(Xfull).sum())\n",
    "\n",
    "X_age = df[\"Age\"].to_numpy().copy()\n",
    "mean_value = np.nanmean(X_age)\n",
    "\n",
    "X_age[np.isnan(X_age)] = mean_value\n",
    "\n",
    "print(X_age)\n",
    "print(yDisease)\n",
    "\n",
    "def add_bias(X):\n",
    "    \"\"\"\n",
    "    Add a column of ones so the model can learn an intercept term.\n",
    "    If X is (n, d), output is (n, d+1).\n",
    "    \"\"\"\n",
    "    return np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "def one_hot(y, K):\n",
    "    \"\"\"\n",
    "    Convert integer labels into one-hot rows.\n",
    "\n",
    "    TODO (completed):\n",
    "    1) Create a zeros matrix (n, K)\n",
    "    2) Set the correct class index to 1 for each row\n",
    "    \"\"\"\n",
    "    y = np.asarray(y).astype(int)\n",
    "    n = y.shape[0]\n",
    "    Y = np.zeros((n, K))\n",
    "    Y[np.arange(n), y] = 1.0\n",
    "    return Y\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Stable softmax for (n, K) scores.\n",
    "\n",
    "    TODO (completed):\n",
    "    1) Subtract max per row for numerical stability\n",
    "    2) Exponentiate and normalize so each row sums to 1\n",
    "    \"\"\"\n",
    "    Z = np.asarray(Z)\n",
    "    Z_shift = Z - np.max(Z, axis=1, keepdims=True)\n",
    "    expZ = np.exp(Z_shift)\n",
    "    return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
    "\n",
    "def softmax_loss_and_grad(Xb, y_int, W, l2=0.0):\n",
    "    \"\"\"\n",
    "    Softmax regression: average cross-entropy loss and gradient.\n",
    "\n",
    "    TODO (completed):\n",
    "    1) Compute scores and softmax probabilities\n",
    "    2) Compute average cross-entropy loss\n",
    "    3) Compute gradient dW\n",
    "    4) Add L2 penalty and gradient on W[1:,:] only\n",
    "    \"\"\"\n",
    "    n = Xb.shape[0]\n",
    "    K = W.shape[1]\n",
    "\n",
    "    scores = Xb @ W\n",
    "    probs = softmax(scores)\n",
    "    Y = one_hot(y_int, K)\n",
    "\n",
    "    eps = 1e-12\n",
    "    p_true = np.sum(probs * Y, axis=1)\n",
    "    loss = np.mean(-np.log(p_true + eps))\n",
    "\n",
    "    dScores = (probs - Y) / n\n",
    "    dW = Xb.T @ dScores\n",
    "\n",
    "    if l2 > 0.0:\n",
    "        loss += (l2 / 2.0) * np.sum(W[1:, :] ** 2)\n",
    "        reg_grad = np.zeros_like(W)\n",
    "        reg_grad[1:, :] = l2 * W[1:, :]\n",
    "        dW += reg_grad\n",
    "\n",
    "    return loss, dW\n",
    "\n",
    "def train_softmax_gd(Xb_train, y_train, Xb_val, y_val, K,\n",
    "                     lr=0.1, steps=3000, l2=0.0, verbose_every=300):\n",
    "    \"\"\"\n",
    "    Train softmax regression with gradient descent.\n",
    "\n",
    "    TODO (completed):\n",
    "    1) Initialize W zeros with shape (d+1, K)\n",
    "    2) Run gradient descent using softmax_loss_and_grad\n",
    "    3) Track train and val loss\n",
    "    4) Return W and history\n",
    "    \"\"\"\n",
    "    d = Xb_train.shape[1]\n",
    "    W = np.zeros((d, K))\n",
    "\n",
    "    train_loss_hist = []\n",
    "    val_loss_hist = []\n",
    "\n",
    "    for t in range(steps):\n",
    "        tr_loss, gradW = softmax_loss_and_grad(Xb_train, y_train, W, l2=l2)\n",
    "        W = W - lr * gradW\n",
    "\n",
    "        va_loss, _ = softmax_loss_and_grad(Xb_val, y_val, W, l2=l2)\n",
    "        train_loss_hist.append(tr_loss)\n",
    "        val_loss_hist.append(va_loss)\n",
    "\n",
    "        if verbose_every is not None and (t % verbose_every == 0 or t == steps - 1):\n",
    "            pass\n",
    "\n",
    "    history = {\"train_loss\": train_loss_hist, \"val_loss\": val_loss_hist}\n",
    "    return W, history\n",
    "\n",
    "def split_train_val_test(X, y, train_size=0.6, val_size=0.2, test_size=0.2, seed=42):\n",
    "    assert abs(train_size + val_size + test_size - 1.0) < 1e-9\n",
    "    n = X.shape[0]\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = rng.permutation(n)\n",
    "    n_train = int(train_size * n)\n",
    "    n_val = int(val_size * n)\n",
    "    train_idx = idx[:n_train]\n",
    "    val_idx = idx[n_train:n_train+n_val]\n",
    "    test_idx = idx[n_train+n_val:]\n",
    "    return X[train_idx], y[train_idx], X[val_idx], y[val_idx], X[test_idx], y[test_idx]\n",
    "\n",
    "def predict_softmax(Xb, W):\n",
    "    probs = softmax(Xb @ W)\n",
    "    return np.argmax(probs, axis=1)\n",
    "\n",
    "avg_acc = 0\n",
    "seed_min = 0\n",
    "seed_max = 100\n",
    "\n",
    "K=7\n",
    "\n",
    "for i in range(seed_min, seed_max + 1):\n",
    "    #X_train, y_train, X_test, y_test = split_train_val_test(X_age, yDisease, seed=i)\n",
    "    Xtr, ytr, Xva, yva, Xte, yte = split_train_val_test(X_age, yDisease, seed=i)\n",
    "    Xbtr = add_bias(Xtr)\n",
    "    Xbva = add_bias(Xva)\n",
    "    Xbte = add_bias(Xte)\n",
    "\n",
    "    w_hat,cost_hist = train_softmax_gd(Xbtr, ytr, Xbva, yva, K=K, lr=0.001, steps=1000, l2=1e-3)\n",
    "\n",
    "    yhat = predict_softmax(Xbte, w_hat)\n",
    "    test_acc = np.mean(yhat == yte)\n",
    "    print(\"Test accuracy (softmax from scratch):\", test_acc)\n",
    "\n",
    "    avg_acc += test_acc\n",
    "\n",
    "avg_acc /= (seed_max - seed_min + 1)\n",
    "\n",
    "print(f'Test Average Accuracy: %{avg_acc * 100:.2f}')\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"dermatology.csv\", sep=\"\\t\", na_values=\"?\", encoding=\"utf-8-sig\")\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "y_val = df.iloc[:, -1]\n",
    "X_val_all = df.iloc[:, :33]\n",
    "X_val_clinc = df.iloc[:, :11]\n",
    "X_val_histo = df.iloc[:, 11:33]"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaNs in X_train: 0\n",
      "[55.          8.         26.         40.         45.         41.\n",
      " 18.         57.         22.         30.         20.         21.\n",
      " 22.         10.         65.         40.         30.         38.\n",
      " 23.         17.          8.         51.         42.         44.\n",
      " 22.         33.         10.         17.         43.         50.\n",
      " 50.         10.         34.         36.29608939 36.29608939 36.29608939\n",
      " 36.29608939 15.         26.         46.         51.         62.\n",
      " 15.         35.         30.         48.         46.         12.\n",
      " 52.         60.         32.         35.         41.         48.\n",
      " 51.         19.         22.         29.         25.         33.\n",
      "  8.         40.         33.         42.         36.         60.\n",
      " 36.         21.         40.         21.         34.         13.\n",
      " 52.         48.         17.         25.         33.         62.\n",
      " 52.         27.         40.         31.         27.         10.\n",
      " 55.         30.         42.         48.         22.         31.\n",
      " 50.         43.         30.         42.         22.         18.\n",
      " 35.         60.         28.         13.         20.         64.\n",
      " 43.         20.         34.         39.         60.         38.\n",
      " 44.         36.         41.         18.         39.         40.\n",
      " 47.         16.         27.         52.         25.          0.\n",
      " 33.         46.          7.         30.         29.         23.\n",
      "  8.         44.         17.         16.         55.         40.\n",
      " 34.         29.         34.         25.         70.         37.\n",
      " 41.         32.         20.         19.         61.         27.\n",
      " 36.         40.         52.         27.         30.         45.\n",
      " 34.         27.         46.         52.         28.         40.\n",
      " 55.         32.         33.         47.         35.         61.\n",
      " 22.         10.         20.         55.         67.         51.\n",
      " 20.         22.         45.         55.         56.         18.\n",
      " 40.         30.         33.         40.         42.         36.\n",
      " 27.         56.         60.         20.          7.         30.\n",
      " 19.         52.         55.         23.         50.         38.\n",
      " 25.         18.         35.         22.         52.         50.\n",
      " 33.         44.         18.         25.         52.         35.\n",
      " 40.         55.         20.         60.         33.         27.\n",
      " 50.         70.         28.         30.         53.         27.\n",
      " 50.         42.         45.         35.         30.         42.\n",
      " 18.         25.         36.         40.         35.         19.\n",
      " 50.         47.         30.         42.         55.         60.\n",
      " 65.         47.         35.         52.         60.          7.\n",
      "  8.         25.         60.         50.         33.         27.\n",
      " 55.         62.         19.         50.         40.         62.\n",
      " 36.         27.         47.         50.         35.         25.\n",
      " 60.         22.         35.         36.         36.29608939 36.29608939\n",
      " 36.29608939 36.29608939 10.         12.          8.         35.\n",
      " 62.         48.         30.         57.         62.         36.\n",
      " 18.         25.         16.         50.         55.         27.\n",
      " 55.         22.         70.         22.         45.         40.\n",
      " 28.         36.         27.         42.         27.         50.\n",
      " 34.          8.         19.         36.         70.         52.\n",
      " 25.         36.         50.         34.         17.         24.\n",
      " 22.         55.         12.         43.         50.         36.\n",
      " 26.         16.         32.         51.         56.         47.\n",
      " 51.         58.         27.         32.         27.         62.\n",
      " 53.         46.         37.         49.         18.         46.\n",
      " 33.         22.         44.         36.         63.         56.\n",
      " 60.         42.         32.         51.         33.         68.\n",
      " 50.          9.         16.         35.         40.         22.\n",
      " 10.          7.         25.          9.         55.         45.\n",
      " 56.         36.         75.         45.         24.         40.\n",
      " 25.         25.         36.         28.         50.         35.        ]\n",
      "[2 1 3 1 3 2 5 3 4 4 1 2 2 1 3 4 2 1 3 5 6 2 5 3 5 1 6 5 2 3 1 2 1 1 4 2 3\n",
      " 2 3 1 2 4 1 2 5 3 4 6 2 3 3 4 1 1 5 1 2 3 4 2 6 1 5 1 2 3 1 4 5 1 2 6 3 5\n",
      " 4 2 2 1 3 5 1 2 2 2 5 1 1 3 1 4 2 2 5 1 3 4 2 5 1 6 2 5 1 2 2 1 4 1 3 1 1\n",
      " 3 5 3 3 5 2 3 4 1 2 5 6 1 1 2 6 3 5 4 1 1 3 5 5 1 4 2 3 1 2 1 1 3 3 3 2 5\n",
      " 4 2 2 1 1 1 5 3 2 3 2 2 4 2 3 6 2 1 1 3 4 3 3 1 1 1 3 1 1 2 3 3 1 1 1 1 6\n",
      " 2 2 2 2 1 3 3 3 1 1 2 3 2 2 2 5 5 5 5 5 1 1 1 1 1 1 1 3 3 3 3 3 3 4 4 4 4\n",
      " 5 5 5 5 5 5 5 2 2 2 2 1 1 1 1 1 1 6 6 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 4 4 4\n",
      " 4 4 4 5 5 5 5 6 6 6 4 4 4 1 1 1 1 1 2 2 4 4 4 1 1 2 2 2 3 3 3 3 1 1 1 1 5\n",
      " 5 5 5 5 3 3 3 4 1 1 4 4 4 1 1 1 3 3 3 3 3 1 1 1 1 4 4 1 1 4 3 3 4 1 1 4 4\n",
      " 5 5 1 1 5 5 3 1 5 5 6 6 4 4 6 6 6 1 1 1 5 5 1 1 1 1 2 2 4 4 3 3 1]\n",
      "Test accuracy (softmax from scratch): 0.2702702702702703\n",
      "Test accuracy (softmax from scratch): 0.3108108108108108\n",
      "Test accuracy (softmax from scratch): 0.22972972972972974\n",
      "Test accuracy (softmax from scratch): 0.24324324324324326\n",
      "Test accuracy (softmax from scratch): 0.2972972972972973\n",
      "Test accuracy (softmax from scratch): 0.28378378378378377\n",
      "Test accuracy (softmax from scratch): 0.24324324324324326\n",
      "Test accuracy (softmax from scratch): 0.35135135135135137\n",
      "Test accuracy (softmax from scratch): 0.2702702702702703\n",
      "Test accuracy (softmax from scratch): 0.24324324324324326\n",
      "Test accuracy (softmax from scratch): 0.24324324324324326\n",
      "Test accuracy (softmax from scratch): 0.2972972972972973\n",
      "Test accuracy (softmax from scratch): 0.33783783783783783\n",
      "Test accuracy (softmax from scratch): 0.2702702702702703\n",
      "Test accuracy (softmax from scratch): 0.28378378378378377\n",
      "Test accuracy (softmax from scratch): 0.36486486486486486\n",
      "Test accuracy (softmax from scratch): 0.32432432432432434\n",
      "Test accuracy (softmax from scratch): 0.2702702702702703\n",
      "Test accuracy (softmax from scratch): 0.33783783783783783\n",
      "Test accuracy (softmax from scratch): 0.2972972972972973\n",
      "Test accuracy (softmax from scratch): 0.24324324324324326\n",
      "Test accuracy (softmax from scratch): 0.36486486486486486\n",
      "Test accuracy (softmax from scratch): 0.35135135135135137\n",
      "Test accuracy (softmax from scratch): 0.21621621621621623\n",
      "Test accuracy (softmax from scratch): 0.3108108108108108\n",
      "Test accuracy (softmax from scratch): 0.25675675675675674\n",
      "Test accuracy (softmax from scratch): 0.2702702702702703\n",
      "Test accuracy (softmax from scratch): 0.1891891891891892\n",
      "Test accuracy (softmax from scratch): 0.28378378378378377\n",
      "Test accuracy (softmax from scratch): 0.28378378378378377\n",
      "Test accuracy (softmax from scratch): 0.22972972972972974\n",
      "Test accuracy (softmax from scratch): 0.21621621621621623\n",
      "Test accuracy (softmax from scratch): 0.3108108108108108\n",
      "Test accuracy (softmax from scratch): 0.35135135135135137\n",
      "Test accuracy (softmax from scratch): 0.35135135135135137\n",
      "Test accuracy (softmax from scratch): 0.2702702702702703\n",
      "Test accuracy (softmax from scratch): 0.2972972972972973\n",
      "Test accuracy (softmax from scratch): 0.2972972972972973\n",
      "Test accuracy (softmax from scratch): 0.28378378378378377\n",
      "Test accuracy (softmax from scratch): 0.3108108108108108\n",
      "Test accuracy (softmax from scratch): 0.40540540540540543\n",
      "Test accuracy (softmax from scratch): 0.32432432432432434\n",
      "Test accuracy (softmax from scratch): 0.33783783783783783\n",
      "Test accuracy (softmax from scratch): 0.2702702702702703\n",
      "Test accuracy (softmax from scratch): 0.33783783783783783\n",
      "Test accuracy (softmax from scratch): 0.28378378378378377\n",
      "Test accuracy (softmax from scratch): 0.3108108108108108\n",
      "Test accuracy (softmax from scratch): 0.28378378378378377\n",
      "Test accuracy (softmax from scratch): 0.32432432432432434\n",
      "Test accuracy (softmax from scratch): 0.2972972972972973\n",
      "Test accuracy (softmax from scratch): 0.3108108108108108\n",
      "Test accuracy (softmax from scratch): 0.36486486486486486\n",
      "Test accuracy (softmax from scratch): 0.3108108108108108\n",
      "Test accuracy (softmax from scratch): 0.20270270270270271\n",
      "Test accuracy (softmax from scratch): 0.33783783783783783\n",
      "Test accuracy (softmax from scratch): 0.24324324324324326\n",
      "Test accuracy (softmax from scratch): 0.24324324324324326\n",
      "Test accuracy (softmax from scratch): 0.32432432432432434\n",
      "Test accuracy (softmax from scratch): 0.28378378378378377\n",
      "Test accuracy (softmax from scratch): 0.3108108108108108\n",
      "Test accuracy (softmax from scratch): 0.28378378378378377\n",
      "Test accuracy (softmax from scratch): 0.3108108108108108\n",
      "Test accuracy (softmax from scratch): 0.3108108108108108\n",
      "Test accuracy (softmax from scratch): 0.33783783783783783\n",
      "Test accuracy (softmax from scratch): 0.28378378378378377\n",
      "Test accuracy (softmax from scratch): 0.32432432432432434\n",
      "Test accuracy (softmax from scratch): 0.2972972972972973\n",
      "Test accuracy (softmax from scratch): 0.2972972972972973\n",
      "Test accuracy (softmax from scratch): 0.24324324324324326\n",
      "Test accuracy (softmax from scratch): 0.3108108108108108\n",
      "Test accuracy (softmax from scratch): 0.32432432432432434\n",
      "Test accuracy (softmax from scratch): 0.28378378378378377\n",
      "Test accuracy (softmax from scratch): 0.33783783783783783\n",
      "Test accuracy (softmax from scratch): 0.33783783783783783\n",
      "Test accuracy (softmax from scratch): 0.24324324324324326\n",
      "Test accuracy (softmax from scratch): 0.2702702702702703\n",
      "Test accuracy (softmax from scratch): 0.17567567567567569\n",
      "Test accuracy (softmax from scratch): 0.2702702702702703\n",
      "Test accuracy (softmax from scratch): 0.33783783783783783\n",
      "Test accuracy (softmax from scratch): 0.2972972972972973\n",
      "Test accuracy (softmax from scratch): 0.2702702702702703\n",
      "Test accuracy (softmax from scratch): 0.33783783783783783\n",
      "Test accuracy (softmax from scratch): 0.2702702702702703\n",
      "Test accuracy (softmax from scratch): 0.28378378378378377\n",
      "Test accuracy (softmax from scratch): 0.2702702702702703\n",
      "Test accuracy (softmax from scratch): 0.3783783783783784\n",
      "Test accuracy (softmax from scratch): 0.33783783783783783\n",
      "Test accuracy (softmax from scratch): 0.2972972972972973\n",
      "Test accuracy (softmax from scratch): 0.36486486486486486\n",
      "Test accuracy (softmax from scratch): 0.36486486486486486\n",
      "Test accuracy (softmax from scratch): 0.32432432432432434\n",
      "Test accuracy (softmax from scratch): 0.3108108108108108\n",
      "Test accuracy (softmax from scratch): 0.28378378378378377\n",
      "Test accuracy (softmax from scratch): 0.2702702702702703\n",
      "Test accuracy (softmax from scratch): 0.3108108108108108\n",
      "Test accuracy (softmax from scratch): 0.40540540540540543\n",
      "Test accuracy (softmax from scratch): 0.3108108108108108\n",
      "Test accuracy (softmax from scratch): 0.32432432432432434\n",
      "Test accuracy (softmax from scratch): 0.3108108108108108\n",
      "Test accuracy (softmax from scratch): 0.33783783783783783\n",
      "Test accuracy (softmax from scratch): 0.3783783783783784\n",
      "Test Average Accuracy: %29.89\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
