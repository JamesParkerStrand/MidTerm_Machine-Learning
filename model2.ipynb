{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-11T23:04:33.394595500Z",
     "start_time": "2026-02-11T23:04:33.358647400Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"dermatology.csv\", sep=\"\\t\", na_values=\"?\", encoding=\"utf-8-sig\")\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "y_val = df.iloc[:, -1]\n",
    "X_val_all = df.iloc[:, :33]\n",
    "X_val_clinc = df.iloc[:, :11]\n",
    "X_val_histo = df.iloc[:, 11:33]\n",
    "\n",
    "\n",
    "def getMapOfFeat(X):\n",
    "    \"\"\"\n",
    "    Maps the feats in X into a map with index\n",
    "    :param X: X value vector\n",
    "    :return: Map with key value pair with [index:feats]\n",
    "    \"\"\"\n",
    "    index = 0\n",
    "    feats = {}\n",
    "    for feat in X.columns:\n",
    "        feats[index] = feat\n",
    "        index += 1\n",
    "    return feats\n",
    "\n",
    "# Maps the index to the features for reclassification.\n",
    "feats_all = getMapOfFeat(X_val_all)\n",
    "feats_clinc = getMapOfFeat(X_val_clinc)\n",
    "feats_histo = getMapOfFeat(X_val_histo)"
   ],
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T23:04:33.408460500Z",
     "start_time": "2026-02-11T23:04:33.396108200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Splitting the dataset 70/30 and initialize the random forest classifier\n",
    "\n",
    "num_of_trees = 300\n",
    "seed = 42\n",
    "\n",
    "X_train_all, X_test_all, y_train, y_test = train_test_split(X_val_all, y_val, test_size=0.3, random_state=seed, stratify=y_val)\n",
    "X_train_Clinic, X_test_clinic, y_train, y_test = train_test_split(X_val_clinc, y_val, test_size=0.3, random_state=seed, stratify=y_val)\n",
    "X_train_histo, X_test_histo, y_train, y_test = train_test_split(X_val_histo, y_val, test_size=0.3, random_state=seed, stratify=y_val)\n",
    "\n",
    "random_forest = RandomForestClassifier(\n",
    "    n_estimators=num_of_trees,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=seed\n",
    ")\n"
   ],
   "id": "9d5f07955952760a",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T23:04:34.610006Z",
     "start_time": "2026-02-11T23:04:33.410495700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fitting and predicting\n",
    "\n",
    "random_forest.fit(X_train_all, y_train)\n",
    "y_pred_all = random_forest.predict(X_test_all)\n",
    "importances_all = random_forest.feature_importances_\n",
    "\n",
    "random_forest.fit(X_train_Clinic, y_train)\n",
    "y_pred_clinic = random_forest.predict(X_test_clinic)\n",
    "importances_clinic = random_forest.feature_importances_\n",
    "\n",
    "random_forest.fit(X_train_histo, y_train)\n",
    "y_pred_histo = random_forest.predict(X_test_histo)\n",
    "importances_histo = random_forest.feature_importances_\n",
    "\n"
   ],
   "id": "18196c23d9155a76",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T23:04:34.667296500Z",
     "start_time": "2026-02-11T23:04:34.644483200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generating a sample top set and probability matrix from seed = 42\n",
    "top_all = np.argsort(importances_all)[::-1][:5]\n",
    "top_all = sorted(top_all)\n",
    "top_features_all = []\n",
    "acc_score_all = accuracy_score(y_test, y_pred_all)\n",
    "for val in top_all:\n",
    "    top_features_all.append(feats_all.get(val))\n",
    "print(f\"Accuracy score for all features: {acc_score_all:.3f}\")\n",
    "print(\"\\nTop features for all features: \\n\", top_features_all)\n",
    "print(\"\\nTop importances for all features: \\n\", importances_all[top_all])"
   ],
   "id": "25829cba70769eb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for all features: 0.973\n",
      "\n",
      "Top features for all features: \n",
      " ['Koebner', 'Fibrosis', 'Clubbing', 'Elongation', 'Thinning']\n",
      "\n",
      "Top importances for all features: \n",
      " [0.0702556  0.08150345 0.0936548  0.06058227 0.07753667]\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T23:04:34.687537600Z",
     "start_time": "2026-02-11T23:04:34.668810800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_clinic = np.argsort(importances_clinic)[::-1][:10]\n",
    "top_clinic = sorted(top_clinic)\n",
    "top_features_clinic = []\n",
    "acc_score_clinic = accuracy_score(y_test, y_pred_clinic)\n",
    "for val in top_clinic:\n",
    "    top_features_clinic.append(feats_clinc.get(val))\n",
    "print(f\"Accuracy score for clinic features: {acc_score_clinic:.3f}\")\n",
    "print(\"\\nTop features for clinic features: \\n\", top_features_clinic)\n",
    "print(\"\\nTop importances for clinic features: \\n\", importances_clinic[top_clinic])"
   ],
   "id": "17541c9fb711ab2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for clinic features: 0.873\n",
      "\n",
      "Top features for clinic features: \n",
      " ['Erythema', 'Scathing', 'Definite Borders', 'Itching', 'Koebner', 'Polygonal', 'Follicular', 'Oral', 'Knee', 'Scalp']\n",
      "\n",
      "Top importances for clinic features: \n",
      " [0.05790426 0.06881744 0.09513762 0.07795595 0.11821216 0.11229631\n",
      " 0.08264651 0.12082599 0.13500864 0.11223009]\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T23:04:34.718488Z",
     "start_time": "2026-02-11T23:04:34.690835300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_histo= np.argsort(importances_histo)[::-1][:10]\n",
    "top_histo = sorted(top_histo)\n",
    "top_features_histo = []\n",
    "acc_score_histo = accuracy_score(y_test, y_pred_histo)\n",
    "for val in top_histo:\n",
    "    top_features_histo.append(feats_histo.get(val))\n",
    "print(f\"Accuracy score for histopathological features: {acc_score_histo:.3f}\")\n",
    "print(\"\\nTop features for histopathological features: \\n\", top_features_histo)\n",
    "print(\"\\nTop importances for histopathological features: \\n\", importances_histo[top_histo])"
   ],
   "id": "684134cba111bb4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for histopathological features: 0.927\n",
      "\n",
      "Top features for histopathological features: \n",
      " ['Melanin', 'PNL', 'Fibrosis', 'Clubbing', 'Elongation', 'Thinning', 'Focal', 'Vacuolisation', 'Spongiosis', 'Band-like']\n",
      "\n",
      "Top importances for histopathological features: \n",
      " [0.05472001 0.06828361 0.08696701 0.11621669 0.06624838 0.08743244\n",
      " 0.04258721 0.04639641 0.08326073 0.04884262]\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T23:04:38.304173100Z",
     "start_time": "2026-02-11T23:04:34.721509300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Getting the average accuracy rating\n",
    "min_seed = 42\n",
    "max_seed = 52\n",
    "\n",
    "avg_acc = 0\n",
    "avg_prob = 0\n",
    "for i in range(min_seed, max_seed, 1):\n",
    "    X_train_all, X_test_all, y_train, y_test = train_test_split(X_val_all, y_val, test_size=0.2, random_state=seed,\n",
    "                                                                stratify=y_val)\n",
    "\n",
    "    random_forest = RandomForestClassifier(\n",
    "    n_estimators=num_of_trees,\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=i\n",
    "    )\n",
    "\n",
    "    random_forest.fit(X_train_all, y_train)\n",
    "\n",
    "    y_pred = random_forest.predict(X_test_all)\n",
    "\n",
    "    avg_acc += accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "avg_acc /= (max_seed - min_seed)\n",
    "print(f'\\nAverage Accuracy: {avg_acc:.2f}')"
   ],
   "id": "5e76176b4b23b401",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Accuracy: 0.96\n"
     ]
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Report Random Forest\n",
    "\n",
    "### What was learned:\n",
    "Due to not have learned about how to implement a random forest classifications yet, I didn't know how to implement the\n",
    "classification model so I utilized ChatGPT to guide me through the usage of the sklearn library. From my understanding\n",
    "of how the random forest classification model works, it puts all the data into a different trees where each tree\n",
    "trains on a dataset sampled with replacement from the original, resulting in each tree seeing a different mix of rows\n",
    " (Some are repeated and some are missing). When the tree is deciding to split, it doesn't use all the features. It\n",
    " only considers a random subset (commonly sqrt(d) features for classification). At each node, it tries different\n",
    " splits like feature_j <= threashold. It also chooses the split that most improves \"purity\" (Gini impurity, or Entropy/\n",
    " Information gain) of the child nodes. The way the random forest classifier predicts is as such: Given a new input,\n",
    " send the input value down every tree. Each tree outputs a class label, the forest returns the class with the\n",
    " majority of the votes. The key parameters that may require adjusting within the RandomForestClassification function is\n",
    "  the n_estimators: which is the number of trees, more usually yields higher accuracy, but it's often much slower,\n",
    " max_depth: which limits tree depth; this parameter can be adjusted if the model is overfiting, min_samples_leaf:\n",
    " makes the leaves more robust, max_features: how many features to try per split (\"sqrt\" is often the standard choice\n",
    " however you also can choose \"log2\", an integer to set a hard limit, a float that represents a fraction of\n",
    " features), lastly class_weight=\"balanced\" if the classes are imbalanced.\n",
    "\n",
    "## Results:\n",
    "This classification model yielded a much higher accuracy when it comes to predicting the disease using the\n",
    "histopathological features as well as the clinical features, than what the batch gradient decent yielded while only\n",
    "using the Age feature. The average accuracy was found to be around %96. The way the average accuracy was found was\n",
    "from looping seed_max - seed_min times. Each loop increased the seed value by 1. That seed value was then used as a\n",
    "parameter for the random_state within the random forest classifier. Each individual accuracy score was calculated\n",
    "using the sklearn accuracy_score function, summed up and divided by the number loops conducted (seed_max - seed_min).\n",
    "The parameters that were used for each of\n",
    " the random forest classifier as follows:\n",
    "   ```\n",
    "    n_estimators=300,\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=(42-52)\n",
    "   ```\n",
    "\n",
    "After conducting 3 separate experiments; one experiment where I trained the model using all the features, another\n",
    "where I trained the model using only the clinic features, and a third one where I trained the model using only\n",
    "the histopathological features, what I have found was that using all the features gave me the greatest accuracy with\n",
    "a %97.2 accuracy rate. The dataset where I only used the histopathological features yielded a %92.7 accuracy while\n",
    "the dataset with only the clinic features yielded an %87.3 accuracy. The top 5 features that yielded the highest\n",
    "probability for all the features are ['Koebner', 'Fibrosis', 'Clubbing', 'Elongation', 'Thinning']. About (0.0702556 + 0\n",
    ".08150345 + 0.0936548 + 0.06058227 + 0.07753667) %38.35 of forests impurity reduction came from splits on these 5\n",
    "features."
   ],
   "id": "73b17160382b84cc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
